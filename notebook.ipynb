{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['DATA_PATH'] = \"/root/work/data\"\n",
    "import matplotlib.pyplot as plt\n",
    "from invoke import task\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "from model import build_dense, build_conv, build_dense_resid, build_ciresan_1, build_ciresan_4\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from lasagne import layers, objectives, updates\n",
    "from lasagnekit.datasets.mnist import MNIST\n",
    "from lasagnekit.datasets.helpers import split\n",
    "from lasagnekit.datasets.infinite_image_dataset import Transform\n",
    "from helpers import iterate_minibatches, flip, rotate_scale, rotate_scale_one, elastic_transform, elastic_transform_one\n",
    "from tabulate import tabulate\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the net...\n"
     ]
    }
   ],
   "source": [
    "c = 1\n",
    "w = 28\n",
    "h = 28\n",
    "learning_rate = theano.shared(np.array(0.01).astype(np.float32))\n",
    "momentum = 0.9\n",
    "batchsize = 128\n",
    "\n",
    "X = T.tensor4()\n",
    "y = T.ivector()\n",
    "\n",
    "net = build_ciresan_4(\n",
    "    w=w, h=w, c=c, \n",
    "    nb_outputs=10)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "net = build_dense(\n",
    "    w=w, h=w, c=c, \n",
    "    nb_hidden=1000, \n",
    "    nb_outputs=10, \n",
    "    nb_blocks=4, layer_per_block=2)\n",
    "\"\"\"\n",
    "#net = build_conv(\n",
    "#\tw=w, h=h, c=c,\n",
    "#\tnb_filters=16,\n",
    "#\tfilter_size=5,\n",
    "#\tnb_outputs=10,\n",
    "#\tnb_blocks=2,\n",
    "#\tlayer_per_block=3,\n",
    "#\tpool=True\n",
    "#)\n",
    "\n",
    "print('Compiling the net...')\n",
    "\n",
    "y_pred = layers.get_output(net, X)\n",
    "y_pred_detm = layers.get_output(net, X, deterministic=True)\n",
    "#predict_fn = theano.function([X], y_pred)\n",
    "\n",
    "loss = objectives.categorical_crossentropy(y_pred, y).mean()\n",
    "\n",
    "loss_detm = objectives.categorical_crossentropy(y_pred, y).mean()\n",
    "y_acc_detm = T.eq(y_pred_detm.argmax(axis=1), y).mean()\n",
    "\n",
    "loss_fn = theano.function([X, y], loss_detm)\n",
    "acc_fn = theano.function([X, y], y_acc_detm)\n",
    "\n",
    "params = layers.get_all_params(net, trainable=True)\n",
    "grad_updates = updates.momentum(loss, params, learning_rate=learning_rate, momentum=momentum)\n",
    "#grad_updates = updates.adam(loss, params, learning_rate=learning_rate)\n",
    "\n",
    "train_fn = theano.function([X, y], loss, updates=grad_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "\n",
    "def preprocess(data):\n",
    "    data = data * 2 - 1\n",
    "    return data.reshape((data.shape[0], c, w, h))\n",
    "\n",
    "train_full = MNIST(which='train')\n",
    "train_full.load()\n",
    "train_full.X = preprocess(train_full.X)\n",
    "#train_full.X = train_full.X[0:128*10]\n",
    "\n",
    "train, valid = split(train_full, test_size=0.16667) # 10000 examples in validation set\n",
    "\n",
    "test = MNIST(which='test')\n",
    "test.load()\n",
    "test.X = preprocess(test.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augment(X):\n",
    "    X = X[:, 0]\n",
    "    X = rotate_scale(X, min_angle=-7, max_angle=7, min_scale=0.85, max_scale=1.15, n_jobs=10)\n",
    "    X = elastic_transform(X, min_alpha=36, max_alpha=38, min_sigma=5, max_sigma=6, n_jobs=10)\n",
    "    X = X[:, None, :, :]\n",
    "    X = X.astype(np.float32)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "lr_decay = 0.998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff80c89b710>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEACAYAAABPiSrXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZmYFc6NhZmYF+/8cjIApgb2uCAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff81b4bc410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_sims = []\n",
    "lr_sim = lr\n",
    "for i in range(100):\n",
    "    lr_sims.append(lr_sim)\n",
    "    lr_sim *= lr_decay\n",
    "plt.plot(lr_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.353868       0.369638      0.89302       0.8892          27.0269        108.22        1\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.244215       0.260651      0.92316       0.9217          28.0912       109.938        2\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.158189       0.174653      0.95206       0.9478          26.7681       121.738        3\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.130271        0.14498       0.9581       0.9574          28.2496       88.6801        4\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.117954       0.130577      0.96274       0.9595          28.2543       143.409        5\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.108046       0.122592      0.96512       0.9611          27.3413        103.22        6\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.097194       0.110359      0.96882        0.968          27.8345       105.235        7\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.085004       0.102211       0.9721       0.9696          27.3832       93.4563        8\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.084485      0.0994343       0.9728       0.9677          28.2854       89.1078        9\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0705828      0.0852599      0.97804       0.9742          27.8575       103.927       10\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0725658       0.087233      0.97716        0.974           27.669       90.8707       11\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0634427      0.0766073      0.98064       0.9768          28.2134       89.8131       12\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0603859       0.074487       0.9811       0.9773          27.6367       107.685       13\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0647983      0.0812802      0.97986       0.9746          27.1865       88.5176       14\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0536435      0.0685387      0.98394       0.9796           28.117       145.431       15\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0612823       0.074767      0.98122       0.9775          27.3806       129.935       16\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0688954      0.0852849      0.97826       0.9727          28.0385       135.587       17\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0615905      0.0779372        0.981       0.9756          28.4405       99.2224       18\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0468831      0.0638518      0.98538       0.9805          27.5591         99.69       19\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0503313      0.0668924      0.98464       0.9801          27.7302       91.8858       20\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0460799      0.0624343      0.98544        0.981          28.8015       95.9718       21\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0518935      0.0705568       0.9833       0.9774          27.7878       90.7816       22\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0462634      0.0618467      0.98528       0.9809          28.3366       99.0327       23\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0414729      0.0604634      0.98702       0.9815          27.6364       99.4963       24\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0428984      0.0577298      0.98608       0.9831          27.5575        156.04       25\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0510485      0.0639183       0.9837       0.9796          28.1188       89.9135       26\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0389967      0.0524782      0.98804       0.9843          27.8102        109.39       27\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0374274      0.0524386      0.98882       0.9843          28.2355       92.3023       28\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0424645      0.0574577      0.98652       0.9833          28.7208       112.296       29\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0412166      0.0557951      0.98644       0.9834           27.564       92.8155       30\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0398648      0.0544308      0.98768       0.9839           28.158       127.621       31\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0391031      0.0567827      0.98778       0.9844          28.2683       133.248       32\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0327461      0.0522497       0.9898       0.9849          27.7953       100.365       33\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0339628      0.0520116       0.9896       0.9851          27.2851       163.033       34\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0343171      0.0544341      0.98884       0.9849          27.7313       91.2015       35\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0335219      0.0490628      0.98948       0.9852           27.901       108.924       36\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0280962      0.0437547      0.99172        0.988          27.7427       100.765       37\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0304301        0.04592      0.99044       0.9854          28.2013        98.765       38\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0349545      0.0504171      0.98948        0.987          28.2676       120.571       39\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0295296       0.047177      0.99096        0.986          27.7487       89.3848       40\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0429625      0.0583375       0.9859       0.9826           27.395       139.484       41\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0479981      0.0642744      0.98452       0.9785          27.5484       119.113       42\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.027264      0.0465586      0.99126       0.9858          27.8047       87.1667       43\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0367298      0.0548219      0.98822       0.9827          27.7388       87.1045       44\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0244019      0.0434974      0.99226       0.9862          28.0125       96.0973       45\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0354068      0.0493113      0.98894       0.9851          28.2691       88.5753       46\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0310809      0.0485337      0.99002       0.9848          27.6085       107.536       47\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0267531      0.0457187      0.99118        0.985           28.793       104.722       48\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0299892      0.0469338       0.9903       0.9857          28.4849       94.8108       49\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0278907      0.0458535      0.99152       0.9859          27.7314       99.1839       50\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.023167      0.0414245      0.99264       0.9869          27.7629       95.1032       51\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0368431      0.0570726      0.98866       0.9833          28.0546       116.146       52\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0247811      0.0456371      0.99188       0.9877          28.2737        143.39       53\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.021054      0.0391977      0.99354       0.9882          27.8063       151.957       54\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0216135      0.0429288      0.99362       0.9876           28.291       100.783       55\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0297104      0.0510825       0.9911       0.9847          27.9447       87.9849       56\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0292415      0.0523709       0.9907       0.9847           28.048       90.7577       57\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0224032      0.0419209      0.99314       0.9884          28.2803       94.7411       58\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0233798      0.0414741      0.99262       0.9875          27.4239       104.751       59\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0238257      0.0433188       0.9925       0.9875          27.9447       85.4713       60\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 100\n",
    "data_augment = True\n",
    "learning_rate.set_value(np.array(lr).astype(np.float32))\n",
    "\n",
    "print('Training...')\n",
    "\n",
    "for epoch in range(1, nb_epochs + 1):\n",
    "    data_aug_time = []\n",
    "    train_time = []\n",
    "    \n",
    "    # Data augmentation\n",
    "    t = time()\n",
    "    if data_augment:\n",
    "        train_X_full = augment(train.X)\n",
    "    else:\n",
    "        train_X_full = train.X\n",
    "    data_aug_time.append(time() - t)\n",
    "\n",
    "    train_y_full = train.y\n",
    "    for train_X, train_y in iterate_minibatches(train_X_full, train_y_full, batchsize):\n",
    "        # Train one mini=batch\n",
    "        t = time()\n",
    "        train_fn(train_X, train_y)\n",
    "        train_time.append(time() - t)\n",
    "    stats = OrderedDict()\n",
    "    stats['train_loss'] = loss_fn(train.X, train.y)\n",
    "    stats['valid_losss'] = loss_fn(valid.X, valid.y)\n",
    "    stats['train_acc'] = acc_fn(train.X, train.y)\n",
    "    stats['valid_acc'] = acc_fn(valid.X, valid.y)\n",
    "    stats['data_aug_time'] = np.sum(data_aug_time)\n",
    "    stats['train_time'] = np.sum(train_time)\n",
    "    stats['epoch'] = epoch\n",
    "    \n",
    "    history.append(stats)\n",
    "    print(tabulate([stats], headers=\"keys\"))\n",
    "    \n",
    "    lr = learning_rate.get_value()\n",
    "    lr = lr * lr_decay\n",
    "    learning_rate.set_value(np.array(lr).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_acc = [h['train_acc'] for h in history]\n",
    "test_acc = [h['valid_acc'] for h in history]\n",
    "plt.plot(train_acc)\n",
    "plt.plot(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_acc = acc_fn(test.X, test.y)\n",
    "print(1-test_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_layers = layers.get_all_layers(net)\n",
    "input_var = all_layers[0].input_var\n",
    "for l in all_layers:\n",
    "    f = theano.function([input_var], layers.get_output(l, input_var))\n",
    "    h = f(train.X)\n",
    "    print(l, h.shape)\n",
    "    plt.hist(h.flatten())\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
