{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['DATA_PATH'] = \"/root/work/data\"\n",
    "import matplotlib.pyplot as plt\n",
    "from invoke import task\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "from model import build_dense, build_conv, build_dense_resid, build_ciresan_1, build_ciresan_4\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from lasagne import layers, objectives, updates\n",
    "from lasagnekit.datasets.mnist import MNIST\n",
    "from lasagnekit.datasets.helpers import split\n",
    "from lasagnekit.datasets.infinite_image_dataset import Transform\n",
    "from helpers import iterate_minibatches, flip, rotate_scale, rotate_scale_one, elastic_transform, elastic_transform_one\n",
    "from tabulate import tabulate\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the net...\n"
     ]
    }
   ],
   "source": [
    "c = 1\n",
    "w = 28\n",
    "h = 28\n",
    "learning_rate = theano.shared(np.array(0.01).astype(np.float32))\n",
    "momentum = 0.9\n",
    "batchsize = 128\n",
    "\n",
    "X = T.tensor4()\n",
    "y = T.ivector()\n",
    "\n",
    "net = build_ciresan_1(\n",
    "    w=w, h=w, c=c, \n",
    "    nb_outputs=10)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "net = build_dense(\n",
    "    w=w, h=w, c=c, \n",
    "    nb_hidden=1000, \n",
    "    nb_outputs=10, \n",
    "    nb_blocks=4, layer_per_block=2)\n",
    "\"\"\"\n",
    "#net = build_conv(\n",
    "#\tw=w, h=h, c=c,\n",
    "#\tnb_filters=16,\n",
    "#\tfilter_size=5,\n",
    "#\tnb_outputs=10,\n",
    "#\tnb_blocks=2,\n",
    "#\tlayer_per_block=3,\n",
    "#\tpool=True\n",
    "#)\n",
    "\n",
    "print('Compiling the net...')\n",
    "\n",
    "y_pred = layers.get_output(net, X)\n",
    "y_pred_detm = layers.get_output(net, X, deterministic=True)\n",
    "#predict_fn = theano.function([X], y_pred)\n",
    "\n",
    "loss = objectives.categorical_crossentropy(y_pred, y).mean()\n",
    "\n",
    "loss_detm = objectives.categorical_crossentropy(y_pred, y).mean()\n",
    "y_acc_detm = T.eq(y_pred_detm.argmax(axis=1), y).mean()\n",
    "\n",
    "loss_fn = theano.function([X, y], loss_detm)\n",
    "acc_fn = theano.function([X, y], y_acc_detm)\n",
    "\n",
    "params = layers.get_all_params(net, trainable=True)\n",
    "grad_updates = updates.momentum(loss, params, learning_rate=learning_rate, momentum=momentum)\n",
    "#grad_updates = updates.adam(loss, params, learning_rate=learning_rate)\n",
    "\n",
    "train_fn = theano.function([X, y], loss, updates=grad_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "\n",
    "def preprocess(data):\n",
    "    data = data * 2 - 1\n",
    "    return data.reshape((data.shape[0], c, w, h))\n",
    "\n",
    "train_full = MNIST(which='train')\n",
    "train_full.load()\n",
    "train_full.X = preprocess(train_full.X)\n",
    "#train_full.X = train_full.X[0:128*10]\n",
    "\n",
    "train, valid = split(train_full, test_size=0.16667) # 10000 examples in validation set\n",
    "\n",
    "test = MNIST(which='test')\n",
    "test.load()\n",
    "test.X = preprocess(test.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augment(X):\n",
    "    X = X[:, 0]\n",
    "    X = rotate_scale(X, min_angle=-15, max_angle=15, min_scale=0.85, max_scale=1.15, n_jobs=10)\n",
    "    X = elastic_transform(X, min_alpha=36, max_alpha=38, min_sigma=5, max_sigma=6, n_jobs=10)\n",
    "    X = X[:, None, :, :]\n",
    "    X = X.astype(np.float32)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f05d631fa10>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f05d491e950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_sims = []\n",
    "lr_sim = lr\n",
    "for i in range(100):\n",
    "    lr_sims.append(lr_sim)\n",
    "    lr_sim *= 0.91\n",
    "plt.plot(lr_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.641376       0.644422      0.81998       0.8187          27.0661        14.801        1\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.366791       0.371847      0.88928       0.8861          27.7048       12.3094        2\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.247427       0.247128      0.93044       0.9309          26.9663       15.9933        3\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.231174       0.233354      0.93116       0.9277          27.5954       13.4716        4\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.249588       0.251378       0.9231       0.9225           27.615       12.5307        5\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "     0.18667       0.186132      0.94498       0.9453          27.8416       19.5239        6\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.195737       0.195973      0.94064       0.9404          27.5001       15.6663        7\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.155876        0.15437      0.95212       0.9536          27.3759       14.9836        8\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.150285       0.151131      0.95706       0.9571           27.813       17.5085        9\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.131216       0.131312      0.96154       0.9605          27.3565       24.4175       10\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.134355       0.135311      0.95994       0.9606          28.1842       17.4187       11\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.132076       0.132347       0.9616       0.9614          27.3989       27.7123       12\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.119934       0.122943      0.96476       0.9637          27.5473       39.1537       13\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.117614       0.120908      0.96588       0.9644          28.9376       27.0778       14\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.128102       0.128688      0.96214       0.9621          27.8458        15.133       15\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.122856       0.122808       0.9645        0.964          27.9887       49.1832       16\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.123782       0.123548      0.96314       0.9637          27.5924       18.1901       17\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.114988       0.115086      0.96602       0.9663          28.1216       15.6121       18\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.120854       0.120278      0.96414       0.9639          28.0234       26.1381       19\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.111005       0.111084      0.96704       0.9678          27.4201       16.8467       20\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.105498       0.104796      0.96898       0.9708          28.3564       14.9267       21\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.111109       0.110573      0.96764       0.9681          28.2066       18.9146       22\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.107135       0.106414      0.96854       0.9699          28.5178       22.2718       23\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0999021       0.100813      0.97078        0.971           27.609       14.2147       24\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.102851       0.104731       0.9703       0.9704          28.3956       12.8164       25\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.105791       0.107511      0.96934       0.9695          28.2186       14.6944       26\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.102952       0.104295       0.9705       0.9698          27.8259        13.493       27\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "    0.096768      0.0979811       0.9725       0.9721          27.8762       13.8786       28\n",
      "  train_loss    valid_losss    train_acc    valid_acc    data_aug_time    train_time    epoch\n",
      "------------  -------------  -----------  -----------  ---------------  ------------  -------\n",
      "   0.0967776      0.0978105      0.97204       0.9712          27.7178       13.7596       29\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 100\n",
    "data_augment = True\n",
    "learning_rate.set_value(np.array(lr).astype(np.float32))\n",
    "\n",
    "print('Training...')\n",
    "\n",
    "for epoch in range(1, nb_epochs + 1):\n",
    "    data_aug_time = []\n",
    "    train_time = []\n",
    "    \n",
    "    # Data augmentation\n",
    "    t = time()\n",
    "    if data_augment:\n",
    "        train_X_full = augment(train.X)\n",
    "    else:\n",
    "        train_X_full = train.X\n",
    "    data_aug_time.append(time() - t)\n",
    "\n",
    "    train_y_full = train.y\n",
    "    for train_X, train_y in iterate_minibatches(train_X_full, train_y_full, batchsize):\n",
    "        # Train one mini=batch\n",
    "        t = time()\n",
    "        train_fn(train_X, train_y)\n",
    "        train_time.append(time() - t)\n",
    "    stats = OrderedDict()\n",
    "    stats['train_loss'] = loss_fn(train.X, train.y)\n",
    "    stats['valid_losss'] = loss_fn(valid.X, valid.y)\n",
    "    stats['train_acc'] = acc_fn(train.X, train.y)\n",
    "    stats['valid_acc'] = acc_fn(valid.X, valid.y)\n",
    "    stats['data_aug_time'] = np.sum(data_aug_time)\n",
    "    stats['train_time'] = np.sum(train_time)\n",
    "    stats['epoch'] = epoch\n",
    "    \n",
    "    history.append(stats)\n",
    "    print(tabulate([stats], headers=\"keys\"))\n",
    "    \n",
    "    lr = learning_rate.get_value()\n",
    "    lr = lr * 0.91\n",
    "    learning_rate.set_value(np.array(lr).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_acc = [h['train_acc'] for h in history]\n",
    "test_acc = [h['valid_acc'] for h in history]\n",
    "plt.plot(train_acc)\n",
    "plt.plot(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_acc = acc_fn(test.X, test.y)\n",
    "print(1-test_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_layers = layers.get_all_layers(net)\n",
    "input_var = all_layers[0].input_var\n",
    "for l in all_layers:\n",
    "    f = theano.function([input_var], layers.get_output(l, input_var))\n",
    "    h = f(train.X)\n",
    "    print(l, h.shape)\n",
    "    plt.hist(h.flatten())\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
